{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYp0bXOFK-hP"
   },
   "source": [
    "# Машинное обучение, ФКН ВШЭ\n",
    "\n",
    "## Практическое задание 8. Метод опорных векторов и аппроксимация ядер\n",
    "\n",
    "### Общая информация\n",
    "Дата выдачи: 05.02.2021\n",
    "\n",
    "Мягкий дедлайн: 01:59MSK 21.02.2021\n",
    "\n",
    "Жесткий дедлайн: 01:59MSK 24.02.2021\n",
    "\n",
    "### Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимальная оценка за работу (без учёта бонусов) — 10 баллов.\n",
    "\n",
    "Сдавать задание после указанного жёсткого срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "\n",
    "### Формат сдачи\n",
    "Задания сдаются через систему anytask. Посылка должна содержать:\n",
    "* Ноутбук homework-practice-08-random-features-Username.ipynb\n",
    "\n",
    "Username — ваша фамилия и имя на латинице именно в таком порядке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY8vT0W_K-hR"
   },
   "source": [
    "### О задании\n",
    "\n",
    "На занятиях мы подробно обсуждали метод опорных векторов (SVM). В базовой версии в нём нет чего-то особенного — мы всего лишь используем специальную функцию потерь, которая не требует устремлять отступы к бесконечности; ей достаточно, чтобы отступы были не меньше +1. Затем мы узнали, что SVM можно переписать в двойственном виде, который, позволяет заменить скалярные произведения объектов на ядра. Это будет соответствовать построению модели в новом пространстве более высокой размерности, координаты которого представляют собой нелинейные модификации исходных признаков.\n",
    "\n",
    "Ядровой SVM, к сожалению, довольно затратен по памяти (нужно хранить матрицу Грама размера $d \\times d$) и по времени (нужно решать задачу условной оптимизации с квадратичной функцией, а это не очень быстро). Мы обсуждали, что есть способы посчитать новые признаки $\\tilde \\varphi(x)$ на основе исходных так, что скалярные произведения этих новых $\\langle \\tilde \\varphi(x), \\tilde \\varphi(z) \\rangle$ приближают ядро $K(x, z)$.\n",
    "\n",
    "Мы будем исследовать аппроксимации методом Random Fourier Features (RFF, также в литературе встречается название Random Kitchen Sinks) для гауссовых ядер. Будем использовать формулы, которые немного отличаются от того, что было на лекциях (мы добавим сдвиги внутрь тригонометрических функций и будем использовать только косинусы, потому что с нужным сдвигом косинус превратится в синус):\n",
    "$$\\tilde \\varphi(x) = (\n",
    "\\cos (w_1^T x + b_1),\n",
    "\\dots,\n",
    "\\cos (w_n^T x + b_n)\n",
    "),$$\n",
    "где $w_j \\sim \\mathcal{N}(0, 1/\\sigma^2)$, $b_j \\sim U[-\\pi, \\pi]$.\n",
    "\n",
    "На новых признаках $\\tilde \\varphi(x)$ мы будем строить любую линейную модель.\n",
    "\n",
    "Можно считать, что это некоторая новая парадигма построения сложных моделей. Можно направленно искать сложные нелинейные закономерности в данных с помощью градиентного бустинга или нейронных сетей, а можно просто нагенерировать большое количество случайных нелинейных признаков и надеяться, что быстрая и простая модель (то есть линейная) сможет показать на них хорошее качество. В этом задании мы изучим, насколько работоспособна такая идея.\n",
    "\n",
    "### Алгоритм\n",
    "\n",
    "Вам потребуется реализовать следующий алгоритм:\n",
    "1. Понизить размерность выборки до new_dim с помощью метода главных компонент.\n",
    "2. Для полученной выборки оценить гиперпараметр $\\sigma^2$ с помощью эвристики (рекомендуем считать медиану не по всем парам объектов, а по случайному подмножеству из где-то миллиона пар объектов): $$\\sigma^2 = \\text{median}_{i, j = 1, \\dots, \\ell, i \\neq j} \\left\\{\\sum_{k = 1}^{d} (x_{ik} - x_{jk})^2 \\right\\}$$\n",
    "3. Сгенерировать n_features наборов весов $w_j$ и сдвигов $b_j$.\n",
    "4. Сформировать n_features новых признаков по формулам, приведённым выше.\n",
    "5. Обучить линейную модель (логистическую регрессию или SVM) на новых признаках.\n",
    "6. Повторить преобразования (PCA, формирование новых признаков) к тестовой выборке и применить модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_sGunb7K-hS"
   },
   "source": [
    "Тестировать алгоритм мы будем на данных Fashion MNIST. Ниже код для их загрузки и подготовки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YyG6dBfjK-hS"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "(x_train_pics, y_train), (x_test_pics, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train_pics.reshape(x_train_pics.shape[0], -1)\n",
    "x_test = x_test_pics.reshape(x_test_pics.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJNN55F7K-hT"
   },
   "source": [
    "__Задание 1. (5 баллов)__\n",
    "\n",
    "Реализуйте алгоритм, описанный выше. Можете воспользоваться шаблоном класса ниже или написать свой интерфейс.\n",
    "\n",
    "Ваша реализация должна поддерживать следующие опции:\n",
    "1. Возможность задавать значения гиперпараметров new_dim (по умолчанию 50) и n_features (по умолчанию 1000).\n",
    "2. Возможность включать или выключать предварительное понижение размерности с помощью метода главных компонент.\n",
    "3. Возможность выбирать тип линейной модели (логистическая регрессия или SVM с линейным ядром).\n",
    "\n",
    "Протестируйте на данных Fashion MNIST, сформированных кодом выше. Если на тесте у вас получилась доля верных ответов не ниже 0.84 с гиперпараметрами по умолчанию, то вы всё сделали правильно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jP8yepx8K-hT"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RFFPipeline(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
    "        \"\"\"        \n",
    "        Implements pipeline, which consists of PCA decomposition,\n",
    "        Random Fourier Features approximation and linear classification model.\n",
    "        \n",
    "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
    "\n",
    "        new_dim, int: PCA output size.\n",
    "        \n",
    "        use_PCA, bool: whether to include PCA preprocessing.\n",
    "        \n",
    "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
    "        \n",
    "        Feel free to edit this template for your preferences.    \n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.use_PCA = use_PCA\n",
    "        self.new_dim = new_dim\n",
    "        self.classifier = classifier\n",
    "        \n",
    "        if self.classifier == 'logreg':\n",
    "            # этот солвер сходится дольше дефолтного, но качество лучше\n",
    "            self.clf = LogisticRegression(solver='sag')\n",
    "        elif self.classifier == 'svm':\n",
    "            self.clf = SVC(kernel='linear')\n",
    "        else:\n",
    "            raise TypeError('Wrong classifier: choose either \"logreg\" or \"svm\"')\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
    "        \"\"\"\n",
    "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
    "        \n",
    "        # сокращаем размерность\n",
    "        if self.use_PCA:\n",
    "            self.pca = PCA(n_components=self.new_dim)\n",
    "            self.pca.fit(X)\n",
    "            X = self.pca.transform(X)\n",
    "        else:\n",
    "            # если не использовать PCA, то выходит низкая точность. Поэтому чиню это нормализацией\n",
    "            self.sc = StandardScaler()\n",
    "            X = self.sc.fit_transform(X)\n",
    "            \n",
    "        # генерируем случайные пары\n",
    "        idx1 = np.random.choice(range(X.shape[0]), size=10**6)\n",
    "        idx2 = np.random.choice(range(X.shape[0]), size=10**6)\n",
    "        \n",
    "        # выходит примерно 20 совпадений, не будем их учитывать\n",
    "        non_coincidence = idx1 != idx2\n",
    "        idx1 = idx1[non_coincidence]\n",
    "        idx2 = idx2[non_coincidence]\n",
    "        \n",
    "        # оцениваем гиперпараметр\n",
    "        sigma2 = np.median(np.sum((X[idx1]-X[idx2])**2, axis=1))\n",
    "        \n",
    "        # генерируем случайные признаки и сохраняем веса\n",
    "        self.w = np.random.normal(loc=0, scale=np.sqrt(1/sigma2), size=(X.shape[1], self.n_features))\n",
    "        self.b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
    "        X = np.cos(np.dot(X, self.w)+self.b)\n",
    "        \n",
    "        self.clf.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Apply pipeline to obtain scores for input data.\n",
    "        \"\"\"\n",
    "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
    "        if self.classifier == 'svm':\n",
    "            raise AttributeError(\"predict_proba is not available for 'svm' classifier\")\n",
    "        \n",
    "        if self.use_PCA:\n",
    "            X = self.pca.transform(X)\n",
    "        else:\n",
    "            self.sc = StandardScaler()\n",
    "            X = self.sc.fit_transform(X)\n",
    "            \n",
    "        X = np.cos(np.dot(X, self.w)+self.b)\n",
    "        y_pred_proba = self.clf.predict_proba(X)\n",
    "        \n",
    "        return y_pred_proba\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Apply pipeline to obtain discrete predictions for input data.\n",
    "        \"\"\"\n",
    "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
    "        \n",
    "        if self.use_PCA:\n",
    "            X = self.pca.transform(X)\n",
    "        else:\n",
    "            self.sc = StandardScaler()\n",
    "            X = self.sc.fit_transform(X)\n",
    "            \n",
    "        X = np.cos(np.dot(X, self.w)+self.b)\n",
    "        y_pred = self.clf.predict(X)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYqQUEi-K-hU"
   },
   "source": [
    "__Задание 2. (3 балла)__\n",
    "\n",
    "Сравните подход со случайными признаками с обучением SVM на исходных признаках. Попробуйте вариант с обычным (линейным) SVM и с ядровым SVM. Ядровой SVM может очень долго обучаться, поэтому можно делать любые разумные вещи для ускорения: брать подмножество объектов из обучающей выборки, например.\n",
    "\n",
    "Сравните подход со случайными признаками с вариантом, в котором вы понижаете размерность с помощью PCA и обучаете градиентный бустинг. Используйте одну из реализаций CatBoost/LightGBM/XGBoost, не забудьте подобрать число деревьев и длину шага.\n",
    "\n",
    "Сделайте выводы — насколько идея со случайными признаками работает? Сравните как с точки зрения качества, так и с точки зрения скорости обучения и применения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def time_and_acc(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Функция возвращает время обучения и точность классификатора на тестовой выборке.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    fit_time = time.time()-start_time\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return fit_time, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model_dict, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Функция сравнивает несколько моделей по скорости обучения и точности. \n",
    "    \"\"\"\n",
    "    models_list = []\n",
    "    accuracy_list = []\n",
    "    fit_time_list = []\n",
    "    for model in model_dict:\n",
    "        clf = model_dict[model]\n",
    "        fit_time, accuracy = time_and_acc(clf, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        models_list.append(model)\n",
    "        accuracy_list.append(accuracy)\n",
    "        fit_time_list.append(fit_time)\n",
    "        \n",
    "    return models_list, accuracy_list, fit_time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "x_train_sc = sc.fit_transform(x_train)\n",
    "x_test_sc = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "model_dict = {'RFF logreg':RFFPipeline(classifier='logreg'),\n",
    "              'RFF svm':RFFPipeline(classifier='svm'),\n",
    "              'SVM linear':SVC(kernel='linear'),\n",
    "              'SVM RBF':SVC(kernel='rbf')}\n",
    "\n",
    "models_list, accuracy_list, fit_time_list = compare_models(model_dict, x_train_sc, y_train, x_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:22:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:24:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:26:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:28:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:34:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:41:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:48:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:58:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:19:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:20:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:22:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:24:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:29:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:34:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:39:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:46:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:53:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:00:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:02:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:03:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:05:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:09:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:13:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:17:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:23:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:29:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:35:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             param_grid={'learning_rate': [0.3, 0.6, 0.9],\n",
       "                         'n_estimators': [100, 500, 1000]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "x_train_pca = pca.fit_transform(x_train)\n",
    "x_test_pca = pca.transform(x_test)\n",
    "\n",
    "params = {'learning_rate':[0.3, 0.6, 0.9], 'n_estimators':[100, 500, 1000]}\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "grid = GridSearchCV(xgb_clf, params, cv=3, scoring='accuracy')\n",
    "grid.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.3, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "# оптимальные параметры\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:53:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "# обучим бустинг с оптимальными параметрами\n",
    "xgb_clf = xgb.XGBClassifier(**grid.best_params_)\n",
    "fit_time, accuracy = time_and_acc(xgb_clf, x_train_pca, y_train, x_test_pca, y_test)\n",
    "\n",
    "# добавим показатели бустинга в списки\n",
    "models_list.append('XGBOOST')\n",
    "accuracy_list.append(accuracy)\n",
    "fit_time_list.append(fit_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Модель</th>\n",
       "      <th>Точность</th>\n",
       "      <th>Время обучения (с)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RFF logreg</td>\n",
       "      <td>0.8727</td>\n",
       "      <td>169.307905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RFF svm</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>644.151668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM linear</td>\n",
       "      <td>0.8370</td>\n",
       "      <td>1591.415372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM RBF</td>\n",
       "      <td>0.8836</td>\n",
       "      <td>536.383041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>0.8812</td>\n",
       "      <td>1106.665228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Модель  Точность  Время обучения (с)\n",
       "0  RFF logreg    0.8727          169.307905\n",
       "1     RFF svm    0.8735          644.151668\n",
       "2  SVM linear    0.8370         1591.415372\n",
       "3     SVM RBF    0.8836          536.383041\n",
       "4     XGBOOST    0.8812         1106.665228"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "res = pd.DataFrame({'Модель':models_list, 'Точность':accuracy_list, 'Время обучения (с)': fit_time_list})\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самая высокая точность у бустинга (с понижением размерности до 50) и у ядрового SVM. Обе линейные модели с аппроксимацией ядра совсем немного отстают в точности от предыдущих двух моделей. По скорости обучения самая лучшая модель - логистическая регрессия на случайных признаках. Вывод: RFF позволяет быстро обучить линейную модель, качество которой сравнимо со сложными моделями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6umjhWuK-hV"
   },
   "source": [
    "__Задание 3. (2 балла)__\n",
    "\n",
    "Проведите эксперименты:\n",
    "1. Помогает ли предварительное понижение размерности с помощью PCA? \n",
    "2. Как зависит итоговое качество от n_features? Выходит ли оно на плато при росте n_features?\n",
    "3. Важно ли, какую модель обучать — логистическую регрессию или SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В следующих двух заданиях я вернул дефолтный солвер, чтобы быстрее все это работало."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_dict = {'new_dim = 50':RFFPipeline(new_dim=50),\n",
    "              'new_dim = 200':RFFPipeline(new_dim=200),\n",
    "              'new_dim = 500':RFFPipeline(new_dim=500),\n",
    "              'use_PCA = False':RFFPipeline(use_PCA=False)}\n",
    "\n",
    "models_list, accuracy_list, fit_time_list = compare_models(model_dict, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Размерность</th>\n",
       "      <th>Точность</th>\n",
       "      <th>Время обучения (с)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new_dim = 50</td>\n",
       "      <td>0.8593</td>\n",
       "      <td>27.627402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new_dim = 200</td>\n",
       "      <td>0.8664</td>\n",
       "      <td>30.092712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new_dim = 500</td>\n",
       "      <td>0.8608</td>\n",
       "      <td>41.228536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>use_PCA = False</td>\n",
       "      <td>0.8597</td>\n",
       "      <td>136.587915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Размерность  Точность  Время обучения (с)\n",
       "0     new_dim = 50    0.8593           27.627402\n",
       "1    new_dim = 200    0.8664           30.092712\n",
       "2    new_dim = 500    0.8608           41.228536\n",
       "3  use_PCA = False    0.8597          136.587915"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "res = pd.DataFrame({'Размерность':models_list, 'Точность':accuracy_list, 'Время обучения (с)': fit_time_list})\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понижение размерности не сильно влияет на качество классификации, но зато позволяет сократить время обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "c2QIHIMbK-hW",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
    "accuracy = []\n",
    "n_features = range(100, 2001, 100)\n",
    "\n",
    "for n in n_features:\n",
    "    clf = RFFPipeline(n_features=n)\n",
    "    clf.fit(x_train, y_train);\n",
    "    acc = accuracy_score(y_test, clf.predict(x_test))\n",
    "    accuracy.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(n_features, accuracy)\n",
    "plt.xlabel('n_features')\n",
    "plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При росте количества случайных признаков качество модели выходит на плато."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы честно сравнить RFF_logreg и RFF_svm, проведем 5 экспериментов и посмотрим на средние результаты. Здесь я снова установил для логистической регресии значение параметра `solver='sag'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "model_dict = {'RFF_logreg':RFFPipeline(classifier='logreg'), 'RFF_svm':RFFPipeline(classifier='svm')}\n",
    "models_list, accuracy_list, fit_time_list = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "    \n",
    "for train_index, _ in kf.split(x_train):\n",
    "    x_train_cur = x_train[train_index]\n",
    "    y_train_cur = y_train[train_index]\n",
    "    models_list_cur, accuracy_list_cur, fit_time_list_cur = compare_models(model_dict, x_train_cur, y_train_cur, x_test, y_test)\n",
    "    \n",
    "    models_list += models_list_cur\n",
    "    accuracy_list += accuracy_list_cur\n",
    "    fit_time_list += fit_time_list_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Модель</th>\n",
       "      <th>Точность</th>\n",
       "      <th>Время обучения (с)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RFF_logreg</td>\n",
       "      <td>0.8788</td>\n",
       "      <td>147.289845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RFF_svm</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>378.809770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RFF_logreg</td>\n",
       "      <td>0.8758</td>\n",
       "      <td>149.231869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RFF_svm</td>\n",
       "      <td>0.8762</td>\n",
       "      <td>378.915178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RFF_logreg</td>\n",
       "      <td>0.8764</td>\n",
       "      <td>162.940516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RFF_svm</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>563.374920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RFF_logreg</td>\n",
       "      <td>0.8792</td>\n",
       "      <td>187.433064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RFF_svm</td>\n",
       "      <td>0.8795</td>\n",
       "      <td>559.370711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RFF_logreg</td>\n",
       "      <td>0.8753</td>\n",
       "      <td>174.078404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RFF_svm</td>\n",
       "      <td>0.8794</td>\n",
       "      <td>517.032061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Модель  Точность  Время обучения (с)\n",
       "0  RFF_logreg    0.8788          147.289845\n",
       "1     RFF_svm    0.8800          378.809770\n",
       "2  RFF_logreg    0.8758          149.231869\n",
       "3     RFF_svm    0.8762          378.915178\n",
       "4  RFF_logreg    0.8764          162.940516\n",
       "5     RFF_svm    0.8810          563.374920\n",
       "6  RFF_logreg    0.8792          187.433064\n",
       "7     RFF_svm    0.8795          559.370711\n",
       "8  RFF_logreg    0.8753          174.078404\n",
       "9     RFF_svm    0.8794          517.032061"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.DataFrame({'Модель':models_list, 'Точность':accuracy_list, 'Время обучения (с)': fit_time_list})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Точность</th>\n",
       "      <th>Время обучения (с)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Модель</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RFF_logreg</th>\n",
       "      <td>0.87710</td>\n",
       "      <td>164.194740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFF_svm</th>\n",
       "      <td>0.87922</td>\n",
       "      <td>479.500528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Точность  Время обучения (с)\n",
       "Модель                                  \n",
       "RFF_logreg   0.87710          164.194740\n",
       "RFF_svm      0.87922          479.500528"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.groupby('Модель').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве нет сильно ощутимой разницы, но логистическая регрессия обучается гораздо быстрее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJqXVuasK-hW"
   },
   "source": [
    "### Бонус"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVDWHCdrK-hX"
   },
   "source": [
    "__Задание 4. (Максимум 2 балла)__\n",
    "\n",
    "Как вы, должно быть, помните с курса МО-1, многие алгоритмы машинного обучения работают лучше, если признаки данных некоррелированы. Оказывается, что для RFF существует модификация, позволяющая получать ортогональные случайные признаки (Orthogonal Random Features, ORF). Об этом методе можно прочитать в [статье](https://proceedings.neurips.cc/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf). Реализуйте класс для вычисления ORF по аналогии с основным заданием. Обратите внимание, что ваш класс должен уметь работать со случаем n_features > new_dim (в статье есть замечание на этот счет). Проведите эксперименты, сравнивающие RFF и ORF, сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
    "from scipy.stats import chi\n",
    "\n",
    "class ORFPipeline(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
    "        \"\"\"        \n",
    "        Implements pipeline, which consists of PCA decomposition,\n",
    "        Random Fourier Features approximation and linear classification model.\n",
    "        \n",
    "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
    "\n",
    "        new_dim, int: PCA output size.\n",
    "        \n",
    "        use_PCA, bool: whether to include PCA preprocessing.\n",
    "        \n",
    "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
    "        \n",
    "        Feel free to edit this template for your preferences.    \n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.use_PCA = use_PCA\n",
    "        self.new_dim = new_dim\n",
    "        self.classifier = classifier\n",
    "        \n",
    "        if self.classifier == 'logreg':\n",
    "            # этот солвер сходится дольше дефолтного, но качество лучше\n",
    "            self.clf = LogisticRegression(solver='sag')\n",
    "        elif self.classifier == 'svm':\n",
    "            self.clf = SVC(kernel='linear')\n",
    "        else:\n",
    "            raise TypeError('Wrong classifier: choose either \"logreg\" or \"svm\"')\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
    "        \"\"\"\n",
    "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
    "                \n",
    "        # сокращаем размерность\n",
    "        if self.use_PCA:\n",
    "            self.pca = PCA(n_components=self.new_dim)\n",
    "            self.pca.fit(X)\n",
    "            X = self.pca.transform(X)\n",
    "        else:\n",
    "            # если не использовать PCA, то выходит низкая точность. Поэтому чиню это нормализацией\n",
    "            self.sc = StandardScaler()\n",
    "            X = self.sc.fit_transform(X)\n",
    "            \n",
    "        # генерируем случайные пары\n",
    "        idx1 = np.random.choice(range(X.shape[0]), size=10**6)\n",
    "        idx2 = np.random.choice(range(X.shape[0]), size=10**6)\n",
    "        \n",
    "        # выходит примерно 20 совпадений, не будем их учитывать\n",
    "        non_coincidence = idx1 != idx2\n",
    "        idx1 = idx1[non_coincidence]\n",
    "        idx2 = idx2[non_coincidence]\n",
    "        \n",
    "        # оцениваем гиперпараметр\n",
    "        sigma2 = np.median(np.sum((X[idx1]-X[idx2])**2, axis=1))\n",
    "        \n",
    "        # генерируем смещение\n",
    "        self.b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
    "        \n",
    "        # сколько раз надо будет догенерировать признаков, если n_features не кратно количеству исходных признаков\n",
    "        # или количеству признаков после снижения размерности.\n",
    "        # это используется и для случая, когдла n_features меньше числа исходных признаков или ...\n",
    "        residual = self.n_features % X.shape[1] \n",
    "        \n",
    "        # в цикле несколько (или один) раз генерируем случайные признаки\n",
    "        self.w = np.zeros((X.shape[1], self.n_features))\n",
    "        for i in range(0, self.n_features, X.shape[1]):\n",
    "            g = np.random.normal(size=(X.shape[1], X.shape[1]))\n",
    "            q, _ = np.linalg.qr(g)\n",
    "            s = np.diag(chi.rvs(X.shape[1], size=X.shape[1]))\n",
    "            w = s.dot(q)/np.sqrt(sigma2)\n",
    "            \n",
    "            # если влезает вся квадратная матрица, добавляем ее, в противном случае - добавляем влезаемое кол-во столбцов\n",
    "            if self.w[:,i:].shape[1] >= X.shape[1]:\n",
    "                self.w[:,i:i+X.shape[1]] = w\n",
    "            else:\n",
    "                self.w[:,i:i+X.shape[1]] = w[:,:residual]\n",
    "\n",
    "            \n",
    "        X = np.cos(np.dot(X, self.w)+self.b)\n",
    "        \n",
    "        self.clf.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Apply pipeline to obtain scores for input data.\n",
    "        \"\"\"\n",
    "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
    "        if self.classifier == 'svm':\n",
    "            raise AttributeError(\"predict_proba is not available for 'svm' classifier\")\n",
    "        \n",
    "        if self.use_PCA:\n",
    "            X = self.pca.transform(X)\n",
    "        else:\n",
    "            X = self.sc.transform(X)\n",
    "            \n",
    "        X = np.cos(np.dot(X, self.w)+self.b)        \n",
    "        \n",
    "        y_pred_proba = self.clf.predict_proba(X)\n",
    "        \n",
    "        return y_pred_proba\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Apply pipeline to obtain discrete predictions for input data.\n",
    "        \"\"\"\n",
    "        # Your code here: (￣▽￣)/♫•*¨*•.¸¸♪\n",
    "        \n",
    "        if self.use_PCA:\n",
    "            X = self.pca.transform(X)\n",
    "        else:\n",
    "            X = self.sc.transform(X)\n",
    "            \n",
    "        X = np.cos(np.dot(X, self.w)+self.b)        \n",
    "        \n",
    "        y_pred = self.clf.predict(X)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично проведем 5 экспериментов для сравнения ORF и RFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "model_dict = {'RFF logreg':RFFPipeline(),\n",
    "              'ORF logreg':ORFPipeline()}\n",
    "\n",
    "models_list, accuracy_list, fit_time_list = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "    \n",
    "for train_index, _ in kf.split(x_train):\n",
    "    x_train_cur = x_train[train_index]\n",
    "    y_train_cur = y_train[train_index]\n",
    "    models_list_cur, accuracy_list_cur, fit_time_list_cur = compare_models(model_dict, x_train_cur, y_train_cur, x_test, y_test)\n",
    "    \n",
    "    models_list += models_list_cur\n",
    "    accuracy_list += accuracy_list_cur\n",
    "    fit_time_list += fit_time_list_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Модель</th>\n",
       "      <th>Точность</th>\n",
       "      <th>Время обучения (с)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RFF logreg</td>\n",
       "      <td>0.8727</td>\n",
       "      <td>146.619525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORF logreg</td>\n",
       "      <td>0.8766</td>\n",
       "      <td>145.827482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RFF logreg</td>\n",
       "      <td>0.8777</td>\n",
       "      <td>146.219107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORF logreg</td>\n",
       "      <td>0.8777</td>\n",
       "      <td>145.536233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RFF logreg</td>\n",
       "      <td>0.8794</td>\n",
       "      <td>150.986940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORF logreg</td>\n",
       "      <td>0.8765</td>\n",
       "      <td>146.279641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RFF logreg</td>\n",
       "      <td>0.8770</td>\n",
       "      <td>145.659967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ORF logreg</td>\n",
       "      <td>0.8773</td>\n",
       "      <td>145.411296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RFF logreg</td>\n",
       "      <td>0.8760</td>\n",
       "      <td>145.775244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ORF logreg</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>145.083532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Модель  Точность  Время обучения (с)\n",
       "0  RFF logreg    0.8727          146.619525\n",
       "1  ORF logreg    0.8766          145.827482\n",
       "2  RFF logreg    0.8777          146.219107\n",
       "3  ORF logreg    0.8777          145.536233\n",
       "4  RFF logreg    0.8794          150.986940\n",
       "5  ORF logreg    0.8765          146.279641\n",
       "6  RFF logreg    0.8770          145.659967\n",
       "7  ORF logreg    0.8773          145.411296\n",
       "8  RFF logreg    0.8760          145.775244\n",
       "9  ORF logreg    0.8783          145.083532"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "res = pd.DataFrame({'Модель':models_list, 'Точность':accuracy_list, 'Время обучения (с)': fit_time_list})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Точность</th>\n",
       "      <th>Время обучения (с)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Модель</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ORF logreg</th>\n",
       "      <td>0.87728</td>\n",
       "      <td>145.627637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFF logreg</th>\n",
       "      <td>0.87656</td>\n",
       "      <td>147.052157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Точность  Время обучения (с)\n",
       "Модель                                  \n",
       "ORF logreg   0.87728          145.627637\n",
       "RFF logreg   0.87656          147.052157"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.groupby('Модель').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В среднем, точность ORF немного выше и обучается быстрее.Трудно назвать эту разницу существенной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pc7-1jmK-hY"
   },
   "source": [
    "__Задание 5. (Максимум 2 балла)__\n",
    "\n",
    "Поэкспериментируйте с функциями для вычисления новых случайных признаков. Не обязательно использовать косинус от скалярного произведения — можно брать знак от него, хэш и т.д. Придумайте побольше вариантов для генерации признаков и проверьте, не получается ли с их помощью добиваться более высокого качества. Также можете попробовать другой классификатор поверх случайных признаков, сравните результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "dWj-O2vjK-hY"
   },
   "outputs": [],
   "source": [
    "# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "homework-practice-08-random-features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
